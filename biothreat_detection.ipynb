{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Biothreat Detection from Disease Reports\n",
        "\n",
        "This notebook extracts potential biothreats from disease reports using LLM-based analysis.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The workflow:\n",
        "1. Loads and chunks disease report text\n",
        "2. Summarizes each chunk to extract outbreak information\n",
        "3. Consolidates summaries to remove duplicates\n",
        "4. Identifies potential biothreats based on epidemiological criteria\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "import textwrap\n",
        "import sys\n",
        "import collections\n",
        "import string\n",
        "import re\n",
        "import ast\n",
        "from rich import print\n",
        "from typing import Optional, List\n",
        "import openai\n",
        "import math\n",
        "from abc import ABC, abstractmethod\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ARGO Wrapper Classes\n",
        "\n",
        "Wrapper classes for interacting with the Argonne Argo LLM service.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_GPT35 = \"gpt35\"\n",
        "MODEL_GPT4 = \"gpt4\"\n",
        "MODEL_GPT4T = \"gpt4turbo\"\n",
        "\n",
        "class ArgoWrapper:\n",
        "    default_url = 'https://apps.inside.anl.gov/argoapi/api/v1/resource/chat/'\n",
        "    # default_url = 'https://apps-test.inside.anl.gov/argoapi/api/v1/resource/chat/'\n",
        "\n",
        "    def __init__(self, \n",
        "                 url = None, \n",
        "                 model = \"gpt4o\", \n",
        "                 user = '.')-> None:\n",
        "        self.url = url\n",
        "        if self.url is None:\n",
        "            self.url = ArgoWrapper.default_url\n",
        "        self.model = model\n",
        "        self.user = user\n",
        "\n",
        "    def invoke(self, prompt_system: str, prompt_user: str, temperature: float = 0.0, top_p: float = 0.95):\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        data = {\n",
        "                \"user\": self.user,\n",
        "                \"model\": self.model,\n",
        "                \"system\": prompt_system,\n",
        "                \"prompt\": [prompt_user],\n",
        "                \"stop\": [],\n",
        "                \"temperature\": temperature,\n",
        "                \"top_p\": top_p\n",
        "        }\n",
        "        # print(f\"[DEBUG] Calling Argo with temperature={temperature}, top_p={top_p}\")\n",
        "        # Log the payload for debugging\n",
        "        # print(f\"DEBUG: Payload being sent to Argo:\\n{json.dumps(data, indent=2)}\")\n",
        "            \n",
        "        data_json = json.dumps(data)    \n",
        "        response = requests.post(self.url, headers=headers, data=data_json)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            parsed = json.loads(response.text)\n",
        "            return parsed\n",
        "        else:\n",
        "            raise Exception(f\"Request failed with status code: {response.status_code}\")\n",
        "\n",
        "class ArgoEmbeddingWrapper:\n",
        "    default_url = \"https://apps-dev.inside.anl.gov/argoapi/api/v1/resource/embed/\"\n",
        "\n",
        "    def __init__(self, url = None, user = os.getenv(\"USER\")) -> None:\n",
        "        self.url = url if url else ArgoEmbeddingWrapper.default_url\n",
        "        self.user = user\n",
        "        #self.argo_embedding_wrapper = argo_embedding_wrapper\n",
        "\n",
        "    def invoke(self, prompts: list):\n",
        "        headers = { \"Content-Type\": \"application/json\" }\n",
        "        data = {\n",
        "            \"user\": self.user,\n",
        "            \"prompt\": prompts\n",
        "        }\n",
        "        data_json = json.dumps(data)\n",
        "        response = requests.post(self.url, headers=headers, data=data_json)\n",
        "        if response.status_code == 200:\n",
        "            parsed = json.loads(response.text)\n",
        "            return parsed\n",
        "        else:\n",
        "            raise Exception(f\"Request failed with status code: {response.status_code}\")\n",
        "\n",
        "    def embed_documents(self, texts):\n",
        "        return self.invoke(texts)\n",
        "\n",
        "    def embed_query(self, query):\n",
        "        return self.invoke(query)[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM Client Configuration\n",
        "\n",
        "Abstract base class and implementations for different LLM services (OpenAI, Argo, etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Config LLM\n",
        "class LLMClient(ABC):\n",
        "    def __init__(self, max_tokens: Optional[int] = None):\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "    @abstractmethod\n",
        "    def call_chat_completion(self, prompt_system: str, prompt_user: str, temperature: float = 0.0, top_p: float = 0.01):\n",
        "        \"\"\"\n",
        "        Call the chat completion with a system prompt (instructions) \n",
        "        and a user prompt (the actual content or question).\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class OpenAIClient(LLMClient):\n",
        "    def __init__(self, api_key: str, api_base: str, model: str):\n",
        "        super().__init__(max_tokens=5000)\n",
        "        self.model = model\n",
        "        self.client = openai.OpenAI(\n",
        "            api_key=api_key,\n",
        "            base_url=api_base,\n",
        "            )\n",
        "\n",
        "    def call_chat_completion(self, prompt_system: str, prompt_user: str, temperature: float = 0.0, top_p: float = 0.95):\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": prompt_system},\n",
        "                    {\"role\": \"user\", \"content\": prompt_user}\n",
        "                ]\n",
        "            )\n",
        "            # print(f'top_p = {top_p}')\n",
        "            return response.choices[0].message.content.strip() if response.choices else \"NO RESPONSE\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            return \"API ERROR\"\n",
        "    \n",
        "        \n",
        "class ArgoClient(LLMClient):\n",
        "    def __init__(self, model: str = \"gpt4o\", user: str = \"minhui.zhu\"):\n",
        "        super().__init__(max_tokens=5000)\n",
        "        self.wrapper = ArgoWrapper(model=model, user=user)\n",
        "\n",
        "    def call_chat_completion(self, prompt_system: str, prompt_user: str, temperature: float = 0.0, top_p: float = 0.95):\n",
        "        \"\"\"\n",
        "        For Argo, we replicate the system+user structure by concatenating \n",
        "        them or by passing them to the wrapper if it supports roles explicitly.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Argo does not use max_tokens, but we pass temperature if needed\n",
        "            response = self.wrapper.invoke(prompt_system, prompt_user, temperature=temperature, top_p=top_p)\n",
        "            # print(f\"[DEBUG] Full response: {response}\")\n",
        "            return response.get(\"response\", \"\").strip()  # Ensure we extract the actual content\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Argo API Error: {e}\")\n",
        "        \n",
        "        \n",
        "        \n",
        "def get_llm_client(service: str, **kwargs):\n",
        "    if service == \"llama\":\n",
        "        return OpenAIClient(api_key=\"CELS\", api_base=\"http://195.88.24.64:80/v1\", **kwargs)\n",
        "    elif service == \"deepseek\":\n",
        "        return OpenAIClient(api_key=\"CELS\", api_base=\"http://66.55.67.65:80/v1\", **kwargs)\n",
        "    elif service == \"globus\":\n",
        "        from inference_auth_token import get_access_token\n",
        "        access_token = get_access_token()\n",
        "        return OpenAIClient(api_key=access_token, api_base=\"https://data-portal-dev.cels.anl.gov/resource_server/sophia/vllm/v1\", **kwargs)\n",
        "    elif service == \"argo\":\n",
        "        return ArgoClient(**kwargs)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown LLM service: {service}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompts\n",
        "\n",
        "System prompts for chunk summarization, consolidation, and biothreat detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROMPTS = {\n",
        "    # 1) Chunk Summarization Prompt\n",
        "    \"chunk_summarization_system\": \"\"\"\n",
        "You are a meticulous data extractor specializing in epidemiology. Your task is to analyze a chunk of a long, potentially unstructured desiease report and extract information for every distinct outbreak.\n",
        "\n",
        "**Core Instructions:**\n",
        "- An outbreak is distinct if it involves a different pathogen/variant, a non-contiguous location, or a separate time period.\n",
        "- **Do not generalize or abridge information.** Your goal is to preserve all specific details provided in the text.\n",
        "\n",
        "**Output Requirements:**\n",
        "- If the time the report was written is stated, include it explicitly, respond with: \n",
        "  \"Time of Report Written: <TimeOfReportWritten>\"\n",
        "- For each distinct outbreak, output in the following format (repeat for each outbreak):\n",
        "  \"Disease Outbreak: <NameOfDisease>  \n",
        "   Cause of Infection: <One sentence summarizing the specific pathogens  and/or suspected transmission vector of this outbreak>  \n",
        "   Location, Time, and Cases: <Combine all available details of the outbreak's location, time, and case counts into a dense, fact-based description. \n",
        "   Capture the most precise location possible, precise dates, and a full breakdown of case count and types (include infected people and any positive environmental samples that indicate potential for spread). \n",
        "   Do not use more than 3 sentences.>  \n",
        "   Trend: <One sentence descriobing the developing trend of the outbreak at the time of report written (If ended? Prediction of spreading? Seasonal outbreak? etc.)>  \n",
        "  \"\n",
        "\"\"\",\n",
        "# 2) Chunk Summarization Prompt\n",
        "    \"summarization_system\": \"\"\"\n",
        "You are a data integrity specialist. You will be given several structured reports from different chunks of the same document. \n",
        "Your task is to de-duplicate the list by merging reports that are about the same outbreak due to text overlap during document chunking, ensuring no details are lost.\n",
        "\n",
        "**Output Requirements:**\n",
        "- At the very beginning, give the time of the report in the format: \n",
        "  \"Time of Report Written: <TimeOfReportWritten>\"\n",
        "- For each distinct outbreak, output in the following format (repeat for each outbreak):\n",
        "  \"Disease Outbreak: <NameOfDisease>  \n",
        "   Cause of Infection: <One sentence summarizing the specific pathogens  and/or suspected transmission vector of this outbreak>  \n",
        "   Location, Time, and Cases: <Combine all available details of the outbreak's location, time, and case counts into a dense, fact-based description. \n",
        "   Capture the most precise location possible, precise dates, and a full breakdown of case count and types ((include infected people and any positive environmental samples that indicate potential for spread). \n",
        "   Do not use more than 3 sentences.>  \n",
        "   Trend: <One sentence descriobing the developing trend of the outbreak at the time of report written (If ended? Prediction of spreading? Seasonal outbreak? etc.)>  \n",
        "  \"\n",
        "\"\"\",\n",
        "    # 3) Final Outbreak Detection Prompt\n",
        "    \"find_risk_system\": \"\"\"\n",
        "You are an expert in epidemiology. Analyze a list of outbreaks from summarized disease report to identify any potential bio threat. \n",
        "\n",
        "**Criteria for \"Potential Bio Threat\":**\n",
        "- Based on the infection time and trend of outbreak, it must be actively spreading and/or potentially emerging around the time of report written (not an ended or purely seasonal outbreak).\n",
        "- Ignore any outbreak primarily caused by foodborne or drug use.\n",
        "- Its pathogen or biological agent (virus, bacterium, fungus, toxin, etc.) satisfies one of the following:\n",
        "    1) a serious threat to humans, animals, or plants that can cause significant harm (e.g., high transmissibility, notable morbidity/mortality, or major societal disruption), or\n",
        "    2) recognized by reputable health organizations (e.g., NIAID, CDC, WHO) as a potential biodefense pathogen or bioterrorism agent\n",
        "\n",
        "    Examples of such threats (not exhaustive):\n",
        "    - Avian influenza virus (highly pathogenic)\n",
        "    - Bacillus anthracis\n",
        "    - Botulinum neurotoxin\n",
        "    - Burkholderia mallei\n",
        "    - Burkholderia pseudomallei\n",
        "    - Ebola virus\n",
        "    - Foot-and-mouth disease virus\n",
        "    - Francisella tularensis\n",
        "    - Marburg virus\n",
        "    - Reconstructed 1918 Influenza virus\n",
        "    - Rinderpest virus\n",
        "    - Toxin-producing strains of Clostridium botulinum\n",
        "    - Variola major virus\n",
        "    - Variola minor virus\n",
        "    - Yersinia pestis\n",
        "    ...\n",
        "\n",
        "**Output Requirements:**\n",
        "- For each outbreak idetified as a Potential Bio Threat, precisely report pathogen (include detailed variant name if available), time (finest time window available), detailed location and active case counts (include infected people and any positive environmental samples that indicate potential for spread). \n",
        "  If there are multiple locations with, list each location with case breakdown separately. \n",
        "  Use the following format (repeat for each distinct outbreak): \n",
        "  \"Potential risk: <NameOfPathogen> Time=<TimeOfOutbreak> Location=<Location1> (Cases=<Full case breakdown at Location1 (e.g., 84 cases, 12 sewage samples)>) Location=<Location2> (Cases=<Full case breakdown at Location2>) ...\"\n",
        "  (If time/location/cases is not found, mark them as \"unreported\".)\n",
        "- If no potential bio threat is detected among all outbreaks, respond with the single word:\n",
        "  \"None\"\n",
        "\n",
        "Note that \"Potential risk\" and \"None\" are mutually exclusive and should not occur together.\n",
        "\"\"\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "\n",
        "Functions for text chunking and result parsing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunk_content(content: str, max_words_per_chunk: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    Splits text into chunks such that each chunk is below the max_tokens_per_chunk limit \n",
        "    (rough heuristic). You could also use a more robust approach with the \n",
        "    'langchain.text_splitter.RecursiveCharacterTextSplitter', etc.\n",
        "    \"\"\"\n",
        "    words = content.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(words):\n",
        "        end = start + max_words_per_chunk\n",
        "        chunk = \" \".join(words[start:end])\n",
        "        chunks.append(chunk)\n",
        "        start = end - 25\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def parse_risk_result(risk_text: str) -> str:\n",
        "    \"\"\"\n",
        "    We expect risk_text to be either:\n",
        "      \"None\"\n",
        "    or\n",
        "      \"Potential risk: <NameOfAgent> infected=<N> location=<L>\"\n",
        "    Possibly missing the infected/location fields => we handle that gracefully.\n",
        "\n",
        "    We return it as-is for counting/frequency, or do some minimal cleanup if needed.\n",
        "    \"\"\"\n",
        "    return risk_text.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Processing Functions\n",
        "\n",
        "Core functions for summarizing reports and extracting biothreat information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_chunked_report(content: str, client: LLMClient, temperature: float = 0.0, top_p: float = 0.95) -> str:\n",
        "    \"\"\"\n",
        "    1) Chunk the content\n",
        "    2) Summarize each chunk\n",
        "    3) Combine chunk summaries into final 'report_summary'\n",
        "    \"\"\"\n",
        "    # Use ~1/3 or 1/4 of max_tokens for safety (to leave room for system instructions).\n",
        "    max_words_per_chunk = (client.max_tokens // 2) if client.max_tokens else 2048\n",
        "    chunked = chunk_content(content, max_words_per_chunk=max_words_per_chunk)\n",
        "\n",
        "    chunk_summaries = []\n",
        "    for i, chunk_text in enumerate(chunked):\n",
        "        summary = client.call_chat_completion(\n",
        "            prompt_system=PROMPTS[\"chunk_summarization_system\"],\n",
        "            prompt_user=chunk_text,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p\n",
        "        )\n",
        "        chunk_summaries.append(summary)\n",
        "\n",
        "    # Combine the chunk summaries\n",
        "    report_summary = \"\\n\".join(chunk_summaries)\n",
        "\n",
        "    if len(chunk_summaries) == 1:\n",
        "        summary_total = chunk_summaries[0]\n",
        "        print(len(chunk_summaries))\n",
        "    # return report_summary\n",
        "    else: \n",
        "        summary_total = client.call_chat_completion(\n",
        "        prompt_system=PROMPTS[\"summarization_system\"],\n",
        "        prompt_user=report_summary,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p\n",
        "    )\n",
        "\n",
        "    return summary_total\n",
        "\n",
        "\n",
        "def extract_outbreak_info(report_summary: str, client: LLMClient, temperature: float = 0.0, top_p: float = 0.95) -> str:\n",
        "    \"\"\"\n",
        "    Runs the final outbreak detection on the combined 'report_summary'.\n",
        "    The system instructions specify how to respond.\n",
        "    Output example: \"Potential risk: Ebola virus Infected=20 Location=Kampala\"\n",
        "                   or \"None\"\n",
        "    \"\"\"\n",
        "    response = client.call_chat_completion(\n",
        "        prompt_system=PROMPTS[\"find_risk_system\"],\n",
        "        prompt_user=report_summary,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p\n",
        "    )\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Detection Function\n",
        "\n",
        "Main function that orchestrates the entire biothreat detection pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_disease(\n",
        "    MMWR: str,\n",
        "    service: str = \"globus\",\n",
        "    model: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "    num_iterations: int = 1,\n",
        "    temperature: float = 0.0,\n",
        "    top_p: float = 0.01\n",
        "    ) -> str:\n",
        "    \"\"\"\n",
        "    1) Read the (potentially very long) disease report.\n",
        "    2) Summarize the text in chunks -> final 'report_summary'.\n",
        "    3) For each iteration:\n",
        "        a) Extract potential threats from 'report_summary'.\n",
        "        b) Print out results in either \"No potential risk\" or \n",
        "           \"Potential risk: <agent> infected=<N> location=<L>\" (compact form).\n",
        "    4) Aggregate answers, find top & second top + their counts.\n",
        "    5) Print final result & return \"+\" if top answer is \"Potential risk...\", else \"-\".\n",
        "    \"\"\"\n",
        "\n",
        "    # Validate the MMWR argument\n",
        "    if not isinstance(MMWR, str) or not MMWR.strip():\n",
        "        raise ValueError(\"MMWR must be a non-empty string.\")\n",
        "    \n",
        "    print(f\"[bold white on blue]Disease Report MMWR:[/bold white on blue] {MMWR}\")\n",
        "\n",
        "    # Construct the input file path dynamically\n",
        "    input_file_path = f\"txts/{MMWR}-H.txt\"\n",
        "\n",
        "    \n",
        "\n",
        "    # Load file\n",
        "    try:\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "            full_report_text = f.read().strip()\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Input file not found at {input_file_path}\")\n",
        "\n",
        "    iteration_results = []\n",
        "    report_summary_list = []\n",
        "    for iteration in range(num_iterations):\n",
        "        # Initialize the LLM client\n",
        "        client = get_llm_client(service=service, model=model)\n",
        "\n",
        "        print(f\"\\n[bold green]Iteration {iteration + 1}/{num_iterations}[/bold green]\")\n",
        "        # Summarize entire text in chunks\n",
        "        report_summary = summarize_chunked_report(full_report_text, client, temperature=temperature, top_p=top_p)\n",
        "        print(f\"[blue]Report summary:[/blue] {report_summary}\")\n",
        "        report_summary_list.append(report_summary)\n",
        "\n",
        "        client = get_llm_client(service=service, model=model)\n",
        "        \n",
        "        # Extract outbreak info from the consolidated summary\n",
        "        risk_result = extract_outbreak_info(report_summary, client, temperature=temperature, top_p=top_p)\n",
        "        risk_result_cleaned = parse_risk_result(risk_result)\n",
        "\n",
        "        # Print the iteration's result in a single, compact line\n",
        "        print(f\"\\n[yellow]Iteration Result:[/yellow] {risk_result_cleaned}\")\n",
        "\n",
        "        iteration_results.append(risk_result_cleaned)\n",
        "\n",
        "\n",
        "    results_dict = {\"MMWR\": MMWR,\n",
        "                    \"report_summary_list\": report_summary_list,\n",
        "                    \"iteration_results\": iteration_results\n",
        "                    }\n",
        "\n",
        "    return results_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Usage\n",
        "\n",
        "Example calls to the detection function with different service configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">Disease Report MMWR:</span> mm6711\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;37;44mDisease Report MMWR:\u001b[0m mm6711\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "Input file not found at txts/mm6711-H.txt",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 33\u001b[0m, in \u001b[0;36mdetect_disease\u001b[0;34m(MMWR, service, model, num_iterations, temperature, top_p)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     34\u001b[0m         full_report_text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\n",
            "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'txts/mm6711-H.txt'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example 1: Using Globus service\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#detect_disease(\"mm6711\", service=\"globus\", model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\", num_iterations=5, temperature=0.0, top_p=0.01)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Example 2: Using Argo service\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mdetect_disease\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmm6711\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt4o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Example 3: Using Llama service\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# detect_disease(\"mm6901\", service=\"llama\", model='meta-llama/Llama-3.3-70B-Instruct', num_iterations=1, temperature=0.0)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Example 4: Using Deepseek service\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# detect_disease(\"mm6901\", service=\"deepseek\", model='deepseekV3', num_iterations=1, temperature=0.0)\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[7], line 36\u001b[0m, in \u001b[0;36mdetect_disease\u001b[0;34m(MMWR, service, model, num_iterations, temperature, top_p)\u001b[0m\n\u001b[1;32m     34\u001b[0m         full_report_text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput file not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m iteration_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     39\u001b[0m report_summary_list \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Input file not found at txts/mm6711-H.txt"
          ]
        }
      ],
      "source": [
        "# Example 1: Using Globus service\n",
        "#detect_disease(\"mm6711\", service=\"globus\", model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\", num_iterations=5, temperature=0.0, top_p=0.01)\n",
        "\n",
        "# Example 2: Using Argo service\n",
        "detect_disease(\"mm6711\", service=\"argo\", model=\"gpt4o\", num_iterations=5, temperature=0.0, top_p=0.01)\n",
        "\n",
        "# Example 3: Using Llama service\n",
        "# detect_disease(\"mm6901\", service=\"llama\", model='meta-llama/Llama-3.3-70B-Instruct', num_iterations=1, temperature=0.0)\n",
        "\n",
        "# Example 4: Using Deepseek service\n",
        "# detect_disease(\"mm6901\", service=\"deepseek\", model='deepseekV3', num_iterations=1, temperature=0.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
